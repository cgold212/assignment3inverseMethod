{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cgold212/assignment3inverseMethod/blob/main/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YWrTV1pbYMx"
      },
      "source": [
        "# Assignment 3: Solving Helmholtz differental equations \n",
        "\n",
        "**Deadline**: 25.01.23\n",
        "\n",
        "**Submission**: Submit a PDF export of the completed notebook as well as the ipynb file. \n",
        "\n",
        "\n",
        "\n",
        "In this assignment, we will build a Physics Informed Neural Networks (PINN) that can solve Helmholtz equation. The Helmholtz equation is a Partial Differential Equation (PDE) that arises to solve physical problems such as wave propogation. Its linear version takes the form of the  partial differential equation $∇^{2}h = -k^{2}h$, where $∇^{2}$ is the Laplace operator, $k$ is the wave number and $h$ is the function, where $h(x, z) = u(x, z) + iv(x, z)$\n",
        "\n",
        "\n",
        "We will explore how leveraging data helps us to solve physics problems, in particular Helmholtz equation. We will use DNNs architectures and the boundary and initial conditions in the loss function,\n",
        "a starter code is given to help with data processing and make it a bit easier.\n",
        "\n",
        "In this assignment you have a chance to build your neural network all by yourself.\n",
        "\n",
        "**Note:** you may modify the starter code as you see fit, including changing the signatures of functions and adding/removing helper functions. However, please make sure that you properly explain what you are doing and why."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.autograd as autograd         \n",
        "from torch import Tensor                  \n",
        "import random \n",
        "\n",
        "!pip install pyDOE\n",
        "from pyDOE import lhs \n",
        "import torch.nn as nn                    \n",
        "import torch.optim as optim         \n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "gzOkdQPmjRa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48169d7e-d519-4c67-ac28-c6ef90fa06eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyDOE in /usr/local/lib/python3.8/dist-packages (0.3.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pyDOE) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from pyDOE) (1.7.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz4PGosQ7q5m",
        "outputId": "9c62f3f0-9b7f-4d0a-b3a4-684add2433ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1. Data (15%)\n",
        "\n",
        "<!-- With any machine learning problem, the first thing that we would want to do\n",
        "is to get an intuitive understanding of what our data looks like. Download the file\n",
        "`??` from the course page on Moodle and upload it to Google Drive.\n",
        "Then, mount Google Drive from your Google Colab notebook: -->\n",
        "In this part you will generate a 2-D input dataset with a collection points that enforce the initial and boundary conditions.\n",
        "Where the computational domain is $x ∈ [-1,1]$ and $z ∈ [0, 1]$.\n",
        "\n",
        "We want to generate an input datasets which contain two aperture\n",
        "\n",
        "Each of the datasets have 256 **equally** spaced spatial points to enforce the initial condition. \n"
      ],
      "metadata": {
        "id": "bCinW_Q-6s19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "X with size (260, 260) and the size of each row is (256,) where each row is from -1 to 1 (i.e x0)\n",
        "Z with size (260, 260) and the size of each column is (256,) where each column is from 0 to 1 (i.e. z0)\n",
        "\"\"\"\n",
        "initial_points = 256\n",
        "half_aperture = 0.2\n",
        "x0 = np.linspace(-1,1,initial_points)  # 256 points between -1 and 1 (260,)\n",
        "z0 = np.linspace(0,1,initial_points)  # 256 points between 0 and 1 (260,)\n",
        "\n",
        "X, Z = np.meshgrid(x0, z0) # (256, 256)"
      ],
      "metadata": {
        "id": "Y-orP5HHSpRq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Z[10] \n"
      ],
      "metadata": {
        "id": "AOIsR-X7YIQ5"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (a) -- 7%\n",
        "\n",
        "Generate the initial condition at $z=0$ i.e. $\\mathcal{u}(\\mathcal{x},\\mathcal{z}=0) = rect(\\frac{z}{2 * HalfAperture})$.\n",
        "\n",
        "Where the middle of each rectangular needs to be at the middle of each half.\n",
        "\n",
        "**Note:** the initial condition needs to be **only** at the first place of the first dimension of the array \n"
      ],
      "metadata": {
        "id": "c_AmYYh0Saf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Exact_u = np.zeros((initial_points, initial_points))\n",
        "Exact_v = np.zeros((initial_points, initial_points))"
      ],
      "metadata": {
        "id": "QcBKiB66UqCM"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.legend_handler import Rectangle\n",
        "# Your code goes here \n",
        "middle1 = -0.5\n",
        "middle2 = 0.5\n",
        "Exact_u[:,0] = np.heaviside(x0-middle1+half_aperture,0.5) - np.heaviside(x0-middle1-half_aperture,0.5) + np.heaviside(x0-middle2+half_aperture,0.5) - np.heaviside(x0-middle2-half_aperture,0.5)\n",
        "Exact_v[:,0] = np.heaviside(x0-middle1+half_aperture,0.5) - np.heaviside(x0-middle1-half_aperture,0.5) + np.heaviside(x0-middle2+half_aperture,0.5) - np.heaviside(x0-middle2-half_aperture,0.5)\n",
        "# print(Exact_u[:,0])\n",
        "# print(Exact_u[100,1])"
      ],
      "metadata": {
        "id": "0qpLz5E8DsBU"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(1, figsize=(18, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolor(x0, z0, Exact_u, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel(r'$x$', fontsize=10)\n",
        "plt.ylabel(r'$z$', fontsize=10)\n",
        "plt.title('Initial condition')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolor(x0, z0, Exact_v, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel(r'$x$', fontsize=10)\n",
        "plt.ylabel(r'$z$', fontsize=10)\n",
        "plt.title('Initial condition')"
      ],
      "metadata": {
        "id": "OUjEVtesVFh8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "da65ee57-6948-4879-86fd-943f6ae5858b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Initial condition')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x360 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "U0 = Exact_u.copy()\n",
        "V0 = Exact_v.copy()\n",
        "\n",
        "u_true = U0.flatten('F')[:,None] \n",
        "v_true = V0.flatten('F')[:, None]"
      ],
      "metadata": {
        "id": "rSl3jnk2U1eu"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We prepare the test data to compare against the solution produced by the PINN."
      ],
      "metadata": {
        "id": "2IMcKMsBl21z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_u_test = np.hstack((X.flatten(order='F')[:,None], Z.flatten(order='F')[:,None]))\n",
        "X_v_test = np.hstack((X.flatten(order='F')[:,None], Z.flatten(order='F')[:,None]))\n",
        "\n",
        "# Domain bounds\n",
        "lb = np.array([-1, 0]) #lower bound of x and z domains\n",
        "ub = np.array([1, 1])  #upper bound of x and z domains"
      ],
      "metadata": {
        "id": "Z5_nceDllzLG"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (b) -- 8%\n",
        "\n",
        "Now we want to generte a quasi-random point set object that produces points using Latin Hypercube sampling (LHS). \n",
        "\n",
        "Latin square is an $n \\times n$ array filled with $n$ different symbols, each occurring exactly once in each row and exactly once in each column. A Latin hypercube is the generalization of this concept to an arbitrary number of dimensions where each sample is the **only** one in each axis of the hyperplane containing it.\n",
        "\n",
        "\n",
        "You can start by reading [API documentation here](https://pythonhosted.org/pyDOE/randomized.html#latin-hypercube).\n",
        "\n",
        "\n",
        "The function `generate_training_data` outputs the training data and the ground truth data, the output data is not normalized. \n",
        "\n",
        "In this part you will need to first to complete `X_f` in order to generate LHS for collection points. Second, reate a function `normalize_data` where the input is the data and the output is the normalized data. \n",
        "\n",
        "**Note:** You can modify the `generate_training_data` function so the output will be normalized data.\n"
      ],
      "metadata": {
        "id": "IZGwMSKHqRRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_training_data(N_u, N_f, X, Z, usol):\n",
        "    \"\"\"\n",
        "    X_f contains the samples inside to boundary condition\n",
        "    X_f shape: (N_f, 2)\n",
        "    Where (N_f, 0) and (N_f, 1) are the samples that maintain  x and z \n",
        "    boundary condition respectivly \n",
        "    \"\"\"\n",
        "    # Your code goes here        \n",
        "    Xidx = np.random.choice(np.arange(X.shape[0]), size=N_u, replace=False)\n",
        "    Zidx = np.random.choice(np.arange(X.shape[0]), size=N_u, replace=False)\n",
        "    X_u_train = np.array([X[1,Xidx],Z[Zidx,1]*0])\n",
        "    X_u_train = X_u_train.T\n",
        "    # print(X_u_train.shape)\n",
        "    # print(X_u_train)\n",
        "    u_train = usol[Xidx,Zidx*0]\n",
        "\n",
        "    # Latin Hypercube sampling for collocation points \n",
        "    X_f = lhs(2, N_f)\n",
        "    X_f = X_f * np.array([X[1,:].max() - X[1,:].min(), Z[:,1].max() - Z[:,1].min()]) + np.array([X[1,:].min(), Z[:,1].min()])\n",
        "    # X_u_train2 = X_u_train.reshape(N_u**2,2)\n",
        "    # print(X_f.shape)\n",
        "    # print(X_u_train2.shape)\n",
        "    # print(X_u_train.shape)\n",
        "    X_f_train = np.vstack((X_f, X_u_train)) # append training points to collocation points \n",
        "    # print(X_f_train.shape)\n",
        "    return X_f_train, X_u_train, u_train "
      ],
      "metadata": {
        "id": "tlg5z5uwLOyU"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate the training data, you can modify `N_f` and `N_u` in order to get better results.\n",
        "\n",
        "**Remember the model inputs type need to be Torch and not numpy**"
      ],
      "metadata": {
        "id": "KGtYe8r-pEtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_u = initial_points\n",
        "N_f = 10000\n",
        "\n",
        "# Training data\n",
        "X_f_u_train, X_u_train, u_train = generate_training_data(N_u, N_f, X, Z, U0)\n",
        "X_f_v_train, X_v_train, v_train = generate_training_data(N_u, N_f, X, Z, V0)\n",
        "indices = np.where(X_f_u_train[:,1]==3)[0]\n",
        "# print(len(indices))\n",
        "# print(X_f_u_train[indices,:].shape)\n",
        "print(np.min(X_u_train))"
      ],
      "metadata": {
        "id": "fplDiK1gXONo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19561a8a-279f-42d1-b474-8e251ce2a913"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ChohFmsGvnrg"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyxs7WDI3zHO"
      },
      "source": [
        "## Question 2. Model architecture (30%)\n",
        "\n",
        "**Note:** You can can train the the same model on $u(x, z)$ and $v(x, z)$ seperatly and combin the output of the two models or you can do it directly on $h(x, z)$. Where $|h(x, z)| = \\sqrt{u(x, z)^{2} + v(x, z)^{2}}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (a) -- 15%\n",
        "Implement a PINN model architecture in PyTorch called `PINN` that will take the data points and will solve the Helmholtz equation.\n",
        "\n",
        "It is recommended that the model will contain 9 hidden layers, start with this number and play with it. In assition, you can play with the number of in/out features of the hidden layers in order to get good results.\n",
        "\n",
        "We build the model in PyTorch. Since PyTorch uses automatic\n",
        "differentiation, we only need to write the *forward pass* of our\n",
        "model. \n"
      ],
      "metadata": {
        "id": "6xEK1lwNLE0A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "dLk8Pzps0CSs"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "class PINN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(PINN, self).__init__() #call __init__ from parent class \n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
        "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.bn4 = nn.BatchNorm1d(hidden_size)\n",
        "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.bn5 = nn.BatchNorm1d(hidden_size)\n",
        "        self.fc6 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.bn6 = nn.BatchNorm1d(hidden_size)\n",
        "        self.fc7 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.bn7 = nn.BatchNorm1d(hidden_size)\n",
        "        self.fc8 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.bn8 = nn.BatchNorm1d(hidden_size)\n",
        "        self.fc9 = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "        init.xavier_uniform_(self.fc1.weight)\n",
        "        init.xavier_uniform_(self.fc2.weight)\n",
        "        init.xavier_uniform_(self.fc3.weight)\n",
        "        init.xavier_uniform_(self.fc4.weight)\n",
        "        init.xavier_uniform_(self.fc5.weight)\n",
        "        init.xavier_uniform_(self.fc6.weight)\n",
        "        init.xavier_uniform_(self.fc7.weight)\n",
        "        init.xavier_uniform_(self.fc8.weight)\n",
        "        init.xavier_uniform_(self.fc9.weight)\n",
        "\n",
        "        init.constant_(self.fc1.bias, 0)\n",
        "        init.constant_(self.fc2.bias, 0)\n",
        "        init.constant_(self.fc3.bias, 0)\n",
        "        init.constant_(self.fc4.bias, 0)\n",
        "        init.constant_(self.fc5.bias, 0)\n",
        "        init.constant_(self.fc6.bias, 0)\n",
        "        init.constant_(self.fc7.bias, 0)\n",
        "        init.constant_(self.fc8.bias, 0)\n",
        "        init.constant_(self.fc9.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = torch.relu(self.bn3(self.fc3(x)))\n",
        "        x = torch.relu(self.bn4(self.fc4(x)))\n",
        "        x = torch.relu(self.bn5(self.fc5(x)))\n",
        "        x = torch.relu(self.bn6(self.fc6(x)))\n",
        "        x = torch.relu(self.bn7(self.fc7(x)))\n",
        "        x = torch.relu(self.bn8(self.fc8(x)))\n",
        "        x = torch.sigmoid(self.fc9(x))\n",
        "        return x\n",
        "    # def forward(self, x):\n",
        "    #     # print(x.shape)\n",
        "    #     x = torch.relu((self.fc1(x)))\n",
        "    #     # print(x.shape)\n",
        "    #     # x = torch.relu((self.fc2(x)))\n",
        "    #     # x = torch.relu((self.fc3(x)))\n",
        "    #     # x = torch.relu((self.fc4(x)))\n",
        "    #     # x = torch.relu((self.fc5(x)))\n",
        "    #     # x = torch.relu((self.fc6(x)))\n",
        "    #     # x = torch.relu((self.fc7(x)))\n",
        "    #     # x = torch.relu((self.fc8(x)))\n",
        "    #     x = self.fc9(x)\n",
        "    #     return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input_size = 2 # x,z coordinates\n",
        "# hidden_size = 256\n",
        "# output_size = 2 # u(x,z) uxx(x,z) uzz(x,z)\n",
        "# model = PINN(input_size, hidden_size, output_size)\n",
        "\n",
        "# batch_size = 10256\n",
        "# data_loader = DataLoader(X_f_u_train, batch_size=batch_size, shuffle=True)\n",
        "# for i, batch in enumerate(data_loader):\n",
        "#   x = batch[:1,:]\n",
        "#   break\n",
        "\n",
        "\n",
        "# x = torch.tensor(x,dtype= torch.float32, requires_grad=True)\n",
        "# print(x.shape)\n",
        "# out = model(x)\n",
        "\n",
        "\n",
        "# y = torch.autograd.grad(out, x, torch.ones(out.shape), create_graph=True, retain_graph=True)\n",
        "# yx = torch.autograd.grad(y, x, torch.ones(out.shape), create_graph=True)\n",
        "# print(y)\n",
        "# print(yx)\n",
        "# # # out.backward(torch.ones(out.shape, dtype=torch.float), retain_graph=True)\n",
        "# # # print(out)\n",
        "# # # first_derivative = autograd.grad(model(batch[:2,:])[0,0], batch[0,:], create_graph=True,allow_unused=True )[0]\n",
        "# # print(111111111)\n",
        "# # x.grad = torch.tensor(x.grad,dtype= torch.float32, requires_grad=True)\n",
        "# # print(x)\n",
        "# # out = model(x)\n",
        "# # y = torch.autograd.functional.hessian(ooo,x)\n",
        "\n",
        "\n",
        "# # print(hessian)"
      ],
      "metadata": {
        "id": "Xc4wQl2_aBNg"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PawxAAX8S1nu"
      },
      "source": [
        "**Please show the model scheme here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (b) -- 15%\n",
        "\n",
        "In this part you will build the loss function which takes as input the model parameters $X$ and $Z$, the initial and boundary conditions $X0$, $Z0$ and $H0$ and return the loss with respect to the learnable parameters.\n",
        "\n",
        "The model is trained by enforcing that given an input the output of the network fulfills the Helmholtz's equation, the boundary conditions, and the initial condition. \n",
        "\n",
        "**Remember:** the loss function of the model needs to be `nn.MSELoss()`, use it when you compute the loss."
      ],
      "metadata": {
        "id": "COW_x_jFLctR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_loss_f(u_pred, v_pred, v_pred_xx_zz, u_pred_xx_zz):\n",
        "                \n",
        "  # Write your code here\n",
        "  # print(111111)\n",
        "  # print(u_pred_xx_zz.shape)\n",
        "  resU = u_pred_xx_zz[:,0] + u_pred_xx_zz[:,1] + u_pred\n",
        "  resV = v_pred_xx_zz[:,0] + v_pred_xx_zz[:,1] + v_pred\n",
        "\n",
        "  resU = resU.pow(2)\n",
        "  resV = resV.pow(2)\n",
        "  res = resU + resV\n",
        "  resU = torch.sqrt(resU)\n",
        "  resV = torch.sqrt(resV)\n",
        "  loss = nn.MSELoss()\n",
        "  L1 = loss(resU,torch.zeros_like(resU))\n",
        "  L2 = loss(resV,torch.zeros_like(resU))\n",
        "  return L1+L2\n",
        "  # return res.sum()\n",
        "\n",
        "# def mse_loss_f(model, X, Z):\n",
        "#     # Convert the indices to coordinates\n",
        "#     # x, y = np.meshgrid(indices[:, 0], indices[:, 1])\n",
        "#     coords = np.stack([X, Z], axis=-1)\n",
        "\n",
        "#     # Reshape coordinates to be compatible with the model\n",
        "#     coords = coords.reshape(-1, 2)\n",
        "#     coords = torch.tensor(coords,dtype=torch.float)\n",
        "#     coords = torch.nn.functional.normalize(coords + 1, p=2, dim=1, eps=1e-12, out=None)\n",
        "#     # print(coords.shape)\n",
        "#     # print(coords)\n",
        "#     # Run the coordinates through the model\n",
        "#     output = model(coords)\n",
        "#     # print(output.shape)\n",
        "#     Uoutput = output[:,0]\n",
        "#     Voutput = output[:,1]\n",
        "\n",
        "#     # Reshape the output to match the original shape of the indices\n",
        "#     Uoutput = Uoutput.reshape(256, 256)\n",
        "#     Voutput = Voutput.reshape(256, 256)\n",
        "\n",
        "#     # Set the output as a torch tensor and set requires_grad to True\n",
        "#     Uoutput = torch.tensor(Uoutput, dtype=torch.float, requires_grad=True)\n",
        "#     Voutput = torch.tensor(Voutput, dtype=torch.float, requires_grad=True)\n",
        "\n",
        "#     # Compute the gradient with respect to x and z coordinates\n",
        "#     Uoutput.requiresGrad = True\n",
        "#     Voutput.requiresGrad = True\n",
        "\n",
        "#     # Compute the first gradient in the x direction\n",
        "#     Ufirst_grad_x = torch.gradient(Uoutput, axis=0)[0]\n",
        "#     Ufirst_grad_z = torch.gradient(Uoutput, axis=1)[0]\n",
        "#     Vfirst_grad_x = torch.gradient(Voutput, axis=0)[0]\n",
        "#     Vfirst_grad_z = torch.gradient(Voutput, axis=1)[0]\n",
        "\n",
        "#     # Compute the second gradient in the x direction\n",
        "#     Usecond_grad_x = torch.gradient(Ufirst_grad_x, axis=0)[0]\n",
        "#     Usecond_grad_z = torch.gradient(Ufirst_grad_z, axis=1)[0]\n",
        "#     Vsecond_grad_x = torch.gradient(Vfirst_grad_x, axis=0)[0]\n",
        "#     Vsecond_grad_z = torch.gradient(Vfirst_grad_z, axis=1)[0]\n",
        "    \n",
        "\n",
        "#     # grad_x = grad_x[:, 0].reshape(256, 256)\n",
        "#     # grad_z = grad_z[:, 1].reshape(256, 256)\n",
        "\n",
        "#     resU = Usecond_grad_x + Usecond_grad_z  + Uoutput\n",
        "#     resV = Vsecond_grad_x + Vsecond_grad_z  + Voutput\n",
        "#     resU = resU.pow(2)\n",
        "#     resV = resV.pow(2)\n",
        "#     res = resU + resV\n",
        "#     res = torch.sqrt(res)\n",
        "#     loss = nn.MSELoss()\n",
        "#     return loss(res,torch.zeros_like(res))\n",
        "\n"
      ],
      "metadata": {
        "id": "qr4pZhB3hTAc"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_loss_h(v_pred, u_pred, Exact_u,Exact_v):\n",
        "                \n",
        "  # Write your code here\n",
        "  loss = nn.MSELoss()\n",
        "  u_pred = u_pred.pow(2)\n",
        "  v_pred = v_pred.pow(2)\n",
        "  h = u_pred + v_pred\n",
        "  # print(h[0])\n",
        "\n",
        "  return loss(Exact_u, torch.sqrt(h))"
      ],
      "metadata": {
        "id": "KutDo-hPhMlU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_loss():\n",
        "  # Write your code here\n",
        "        \n",
        "  return ..."
      ],
      "metadata": {
        "id": "qfKT4YiJhLG3"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-VGXelUTM-F"
      },
      "source": [
        "## Question 3. Training (40%)\n",
        "\n",
        "Now, we will write the functions required to train the PyTorch model using the `Adam optimizer` and the `mse loss` Q2-part (b). You can modify the optimizer parameters.\n",
        "\n",
        "Keep in mind that our task is to solve the Helmholtz equation.\n",
        "\n",
        "### Part (a) -- 30%\n",
        "\n",
        "Complete the function `train_model`, and use it to train your PyTorch MLP model.\n",
        "\n",
        "**Note:** You can change the `Adam` optimizer to whatever optimizer you want which gives a good results. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "# def xavier_init(model):\n",
        "#     for name, param in model.named_parameters():\n",
        "#         if \"weight\" in name:\n",
        "#             init.xavier_uniform_(param)\n",
        "#         elif \"bias\" in name:\n",
        "#             init.constant_(param, 0)\n",
        "\n",
        "# Create a data loader\n",
        "batch_size = 10256\n",
        "data_loader = DataLoader(X_f_u_train, batch_size=batch_size, shuffle=True)\n",
        "# Define the network\n",
        "input_size = 2 # x,z coordinates\n",
        "hidden_size = 256\n",
        "output_size = 2 # u(x,z) uxx(x,z) uzz(x,z)\n",
        "model = PINN(input_size, hidden_size, output_size).to(device)\n",
        "\n",
        "Exact_u = torch.tensor(Exact_u,dtype=torch.float).to(device)\n",
        "Exact_v = torch.tensor(Exact_v,dtype=torch.float).to(device)\n",
        "\n",
        "num_epochs = 2500\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=50e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5, amsgrad=False)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99, last_epoch=-1)\n",
        "\n",
        "\n",
        "def train_model(model, data_loader, num_epochs, mse_loss_f, mse_loss_h, optimizer):\n",
        "  start_time = time.time()\n",
        "  print('Training start time: %.2f' % (start_time))\n",
        "  train_losses = []\n",
        "  train_losses_iter = []\n",
        "  n_iters = 0\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "        \n",
        "        # set training parameter\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        total_correct = 0\n",
        "        total_instances = 0        \n",
        "        n_iters = 0\n",
        "        if epoch % 10 == 0:\n",
        "          scheduler.step()\n",
        "          # if epoch == 200 :\n",
        "          #   optimizer = optim.Adam(model.parameters(), lr=1e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5, amsgrad=False)\n",
        "\n",
        "        # Iterate over batches\n",
        "        for i, batch in enumerate(data_loader):\n",
        "          n_iters += 1\n",
        "          optimizer.zero_grad()\n",
        "          indices = np.where(batch[:,1]==0)[0]\n",
        "          np_batch = batch[indices,0].detach().numpy()\n",
        "          idx = np.array([])\n",
        "          for n in range(len(indices)):\n",
        "            idx = np.append(idx,np.where( X[0,:]== np_batch[n]))\n",
        "            # print(len(indices))\n",
        "          # batch = torch.nn.functional.normalize(batch + 1, p=2, dim=1, eps=1e-12, out=None)\n",
        "          batch = torch.tensor(batch,dtype=torch.float32, requires_grad=True).to(device)\n",
        "          # print(batch.shape)\n",
        "\n",
        "          temp = model(batch)\n",
        "          v_pred = temp[:,1]\n",
        "          u_pred = temp[:,0]\n",
        "          v_pred_x_z = torch.autograd.grad(v_pred, batch, torch.ones(v_pred.shape).to(device), create_graph=True, retain_graph=True)\n",
        "          v_pred_x_z = torch.cat(v_pred_x_z,0)\n",
        "          v_pred_xx_zz = torch.autograd.grad(v_pred_x_z, batch, torch.ones(v_pred_x_z.shape), create_graph=True)\n",
        "          v_pred_xx_zz = torch.cat(v_pred_xx_zz,0)\n",
        "          u_pred_x_z = torch.autograd.grad(u_pred, batch, torch.ones(u_pred.shape).to(device), create_graph=True, retain_graph=True)\n",
        "          u_pred_x_z = torch.cat(u_pred_x_z,0)\n",
        "          u_pred_xx_zz = torch.autograd.grad(u_pred_x_z, batch, torch.ones(u_pred_x_z.shape).to(device), create_graph=True)\n",
        "          u_pred_xx_zz = torch.cat(u_pred_xx_zz,0)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          if n_iters % 1 == 0:\n",
        "            print('Iter - %d '%(n_iters))\n",
        "            print('batch' , batch[0,:])\n",
        "            print('grad',u_pred_xx_zz[0,:],u_pred[0])\n",
        "            # print(u_pred.shape)\n",
        "            print('u side',u_pred_xx_zz[0,0] + u_pred_xx_zz[0,1] + u_pred[0])\n",
        "\n",
        "          \n",
        "          \n",
        "          # uzz_pred = temp[:,2]\n",
        "          if len(indices):\n",
        "            loss_f = mse_loss_f(u_pred, v_pred, v_pred_xx_zz, u_pred_xx_zz)\n",
        "            indices = torch.tensor(indices,dtype=torch.int).numpy()\n",
        "            idx = torch.tensor(idx,dtype=torch.int).numpy()\n",
        "            # print(u_pred[indices])\n",
        "            # print(Exact_u[idx,0])\n",
        "            loss_h = mse_loss_h(v_pred[indices], u_pred[indices], Exact_u[idx,0],Exact_v[idx,0])\n",
        "            alpha = loss_h/(loss_f+loss_h)\n",
        "            print('alpha- %5f'%(alpha))\n",
        "            print('insid',((1-alpha)*loss_f)*1e4)\n",
        "            print('bound',alpha*loss_h*1e4)\n",
        "            loss = ((alpha)*loss_h + (1-alpha)*loss_f)*1e4\n",
        "          else:\n",
        "            loss = mse_loss_f(u_pred, v_pred, v_pred_xx_zz, u_pred_xx_zz)\n",
        "            \n",
        "          loss.backward() \n",
        "          optimizer.step()\n",
        "          train_loss += loss.item()\n",
        "          train_losses_iter.append(loss.item()/data_loader.batch_size)\n",
        "          if n_iters % 1 == 0:\n",
        "            print('Iter - %d Train loss - %f'%(epoch , loss.item()/data_loader.batch_size))\n",
        "\n",
        "  # Write your code here\n",
        "  \n",
        "  elapsed = time.time() - start_time\n",
        "  print('Training end time: %.2f, Training time: %.2f' % (time.time(), elapsed))\n",
        "  return train_losses_iter, model\n",
        "\n",
        "train_losses, trained_model = train_model(model, data_loader, num_epochs, mse_loss_f, mse_loss_h, optimizer)"
      ],
      "metadata": {
        "id": "qj6mazyoS4_5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2b4d36d9-8ca9-4a49-beef-9a4049f8a55b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training start time: 1675354026.91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-bb903318cfb5>:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = torch.tensor(batch,dtype=torch.float32, requires_grad=True).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter - 1 \n",
            "batch tensor([-0.2972,  0.8230], grad_fn=<SliceBackward0>)\n",
            "grad tensor([-2.6114e-05, -2.5872e-05], grad_fn=<SliceBackward0>) tensor(0.5098, grad_fn=<SelectBackward0>)\n",
            "u side tensor(0.5098, grad_fn=<AddBackward0>)\n",
            "alpha- 0.404744\n",
            "insid tensor(4147.1733, grad_fn=<MulBackward0>)\n",
            "bound tensor(1917.3666, grad_fn=<MulBackward0>)\n",
            "Iter - 0 Train loss - 0.591316\n",
            "Iter - 1 \n",
            "batch tensor([-0.9291,  0.6221], grad_fn=<SliceBackward0>)\n",
            "grad tensor([1.5355e-06, 1.0929e-06], grad_fn=<SliceBackward0>) tensor(0.5686, grad_fn=<SelectBackward0>)\n",
            "u side tensor(0.5686, grad_fn=<AddBackward0>)\n",
            "alpha- 0.204090\n",
            "insid tensor(5071.3306, grad_fn=<MulBackward0>)\n",
            "bound tensor(333.4546, grad_fn=<MulBackward0>)\n",
            "Iter - 1 Train loss - 0.526988\n",
            "Iter - 1 \n",
            "batch tensor([0.2720, 0.0641], grad_fn=<SliceBackward0>)\n",
            "grad tensor([3.2714e-05, 1.0757e-05], grad_fn=<SliceBackward0>) tensor(0.1917, grad_fn=<SelectBackward0>)\n",
            "u side tensor(0.1917, grad_fn=<AddBackward0>)\n",
            "alpha- 0.422658\n",
            "insid tensor(3187.8271, grad_fn=<MulBackward0>)\n",
            "bound tensor(1708.4734, grad_fn=<MulBackward0>)\n",
            "Iter - 2 Train loss - 0.477408\n",
            "Iter - 1 \n",
            "batch tensor([-0.5292,  0.6103], grad_fn=<SliceBackward0>)\n",
            "grad tensor([ 7.7204e-06, -1.1084e-05], grad_fn=<SliceBackward0>) tensor(0.3686, grad_fn=<SelectBackward0>)\n",
            "u side tensor(0.3686, grad_fn=<AddBackward0>)\n",
            "alpha- 0.277983\n",
            "insid tensor(3468.8926, grad_fn=<MulBackward0>)\n",
            "bound tensor(514.2015, grad_fn=<MulBackward0>)\n",
            "Iter - 3 Train loss - 0.388367\n",
            "Iter - 1 \n",
            "batch tensor([-0.5394,  0.3750], grad_fn=<SliceBackward0>)\n",
            "grad tensor([1.1383e-05, 3.4320e-05], grad_fn=<SliceBackward0>) tensor(0.4158, grad_fn=<SelectBackward0>)\n",
            "u side tensor(0.4158, grad_fn=<AddBackward0>)\n",
            "alpha- 0.309691\n",
            "insid tensor(2791.8440, grad_fn=<MulBackward0>)\n",
            "bound tensor(561.9003, grad_fn=<MulBackward0>)\n",
            "Iter - 4 Train loss - 0.327003\n",
            "Iter - 1 \n",
            "batch tensor([0.2951, 0.0096], grad_fn=<SliceBackward0>)\n",
            "grad tensor([ 1.2892e-07, -2.2029e-05], grad_fn=<SliceBackward0>) tensor(0.3890, grad_fn=<SelectBackward0>)\n",
            "u side tensor(0.3890, grad_fn=<AddBackward0>)\n",
            "alpha- 0.351827\n",
            "insid tensor(2237.0044, grad_fn=<MulBackward0>)\n",
            "bound tensor(659.0903, grad_fn=<MulBackward0>)\n",
            "Iter - 5 Train loss - 0.282381\n",
            "Iter - 1 \n",
            "batch tensor([0.1445, 0.1257], grad_fn=<SliceBackward0>)\n",
            "grad tensor([-3.8317e-06, -3.8027e-06], grad_fn=<SliceBackward0>) tensor(0.3078, grad_fn=<SelectBackward0>)\n",
            "u side tensor(0.3078, grad_fn=<AddBackward0>)\n",
            "alpha- 0.291808\n",
            "insid tensor(2133.0730, grad_fn=<MulBackward0>)\n",
            "bound tensor(362.1571, grad_fn=<MulBackward0>)\n",
            "Iter - 6 Train loss - 0.243295\n",
            "Iter - 1 \n",
            "batch tensor([0.7512, 0.8237], grad_fn=<SliceBackward0>)\n",
            "grad tensor([-9.2899e-07,  7.1263e-06], grad_fn=<SliceBackward0>) tensor(0.5619, grad_fn=<SelectBackward0>)\n",
            "u side tensor(0.5619, grad_fn=<AddBackward0>)\n",
            "alpha- 0.315964\n",
            "insid tensor(1811.9056, grad_fn=<MulBackward0>)\n",
            "bound tensor(386.5911, grad_fn=<MulBackward0>)\n",
            "Iter - 7 Train loss - 0.214362\n",
            "Iter - 1 \n",
            "batch tensor([0.1987, 0.0607], grad_fn=<SliceBackward0>)\n",
            "grad tensor([3.6863e-06, 1.1649e-06], grad_fn=<SliceBackward0>) tensor(0.2927, grad_fn=<SelectBackward0>)\n",
            "u side tensor(0.2927, grad_fn=<AddBackward0>)\n",
            "alpha- 0.360740\n",
            "insid tensor(1496.8636, grad_fn=<MulBackward0>)\n",
            "bound tensor(476.6671, grad_fn=<MulBackward0>)\n",
            "Iter - 8 Train loss - 0.192427\n",
            "Iter - 1 \n",
            "batch tensor([0.6915, 0.1463], grad_fn=<SliceBackward0>)\n",
            "grad tensor([-5.1632e-07, -1.9595e-06], grad_fn=<SliceBackward0>) tensor(0.2887, grad_fn=<SelectBackward0>)\n",
            "u side tensor(0.2887, grad_fn=<AddBackward0>)\n",
            "alpha- 0.340483\n",
            "insid tensor(1383.4253, grad_fn=<MulBackward0>)\n",
            "bound tensor(368.7170, grad_fn=<MulBackward0>)\n",
            "Iter - 9 Train loss - 0.170841\n",
            "Iter - 1 \n",
            "batch tensor([-0.9071,  0.9503], grad_fn=<SliceBackward0>)\n",
            "grad tensor([-3.4472e-06,  1.4908e-05], grad_fn=<SliceBackward0>) tensor(1., grad_fn=<SelectBackward0>)\n",
            "u side tensor(1.0000, grad_fn=<AddBackward0>)\n",
            "alpha- 0.315939\n",
            "insid tensor(1286.3121, grad_fn=<MulBackward0>)\n",
            "bound tensor(274.3861, grad_fn=<MulBackward0>)\n",
            "Iter - 10 Train loss - 0.152174\n",
            "Iter - 1 \n",
            "batch tensor([-0.1092,  0.8072], grad_fn=<SliceBackward0>)\n",
            "grad tensor([ 2.6000e-06, -1.2351e-05], grad_fn=<SliceBackward0>) tensor(0.1457, grad_fn=<SelectBackward0>)\n",
            "u side tensor(0.1457, grad_fn=<AddBackward0>)\n",
            "alpha- 0.334316\n",
            "insid tensor(1105.3513, grad_fn=<MulBackward0>)\n",
            "bound tensor(278.7902, grad_fn=<MulBackward0>)\n",
            "Iter - 11 Train loss - 0.134959\n",
            "Iter - 1 \n",
            "batch tensor([-0.1524,  0.0154], grad_fn=<SliceBackward0>)\n",
            "grad tensor([ 7.6125e-06, -1.5456e-05], grad_fn=<SliceBackward0>) tensor(0.1192, grad_fn=<SelectBackward0>)\n",
            "u side tensor(0.1192, grad_fn=<AddBackward0>)\n",
            "alpha- 0.366713\n",
            "insid tensor(925.2321, grad_fn=<MulBackward0>)\n",
            "bound tensor(310.2423, grad_fn=<MulBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-bb903318cfb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_losses_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_loss_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_loss_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-bb903318cfb5>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, num_epochs, mse_loss_f, mse_loss_h, optimizer)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse_loss_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_pred_xx_zz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_pred_xx_zz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m           \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 250\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5, amsgrad=False)\n",
        "train_losses, trained_model = train_model(trained_model, data_loader, num_epochs, mse_loss_f, mse_loss_h, optimizer)"
      ],
      "metadata": {
        "id": "SY-T4847us_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_model_output(model, indices,X, Z,x0, z0):\n",
        "    # Convert the indices to coordinates\n",
        "    # x, y = np.meshgrid(indices[:, 0], indices[:, 1])\n",
        "    coords = np.stack([X, Z], axis=-1)\n",
        "    print(coords[1,1,:])\n",
        "\n",
        "    # Reshape coordinates to be compatible with the model\n",
        "    coords = coords.reshape(-1, 2)\n",
        "    coords = torch.tensor(coords,dtype=torch.float)\n",
        "    coords = torch.nn.functional.normalize(coords + 1, p=2, dim=1, eps=1e-12, out=None).to(device)\n",
        "    # print(coords.shape)\n",
        "    # print(coords)\n",
        "    # Run the coordinates through the model\n",
        "    output = model(coords)\n",
        "    # print(output.shape)\n",
        "    Uoutput = output[:,0]\n",
        "    Voutput = output[:,1]\n",
        "    # print(Voutput.shape)\n",
        "    # Reshape the output to match the original shape of the indices\n",
        "    Uoutput = Uoutput.reshape(256, 256)\n",
        "    Voutput = Voutput.reshape(256, 256)\n",
        "\n",
        "    Uoutput = Uoutput.pow(2)\n",
        "    Voutput = Voutput.pow(2)\n",
        "    output = Uoutput + Voutput\n",
        "    output = torch.sqrt(output).to('cpu')\n",
        "\n",
        "    # Plot the output as an image\n",
        "    plt.pcolor(x0, z0, output.detach().numpy(), cmap='jet')\n",
        "    # plt.imshow(output.detach().numpy(), cmap='jet')\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "indices = np.indices((256, 256)).T.reshape(-1, 2)\n",
        "plot_model_output(trained_model, indices,X,Z,x0, z0)\n"
      ],
      "metadata": {
        "id": "qSGM1fmnHLIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yC7qu_fVJgCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (b) -- 10%\n",
        "Complete the `plot_learning_curve` function and plot the learning curve, include your plot in your PDF submission."
      ],
      "metadata": {
        "id": "7jcFzcFISRbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curve(train_losses):\n",
        "    \"\"\"\n",
        "    Plot the learning curve.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    plt.plot(train_losses[100:], label='train')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_learning_curve(train_losses)"
      ],
      "metadata": {
        "id": "j9Efjmu4SwpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hDl58XJX68j"
      },
      "source": [
        "## Question 4. Testing (15%)\n",
        "\n",
        "### Part (a) -- 7%\n",
        "\n",
        "Complete the function `test` and use it to print the test error of your model, separately for the test set. \n",
        "\n",
        "Do this by choosing the model architecture and hyperparameters that produces the best validation loss. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, X_test_tensor):\n",
        "                \n",
        "  u_pred = model.forward(X_test_tensor)\n",
        "        \n",
        "  error_vec = ...  # TODO: complete this line. It should be relative L2 Norm of the error (Vector)\n",
        "        \n",
        "  u_pred = np.reshape(u_pred.detach().numpy(), (256,256), order='F') \n",
        "        \n",
        "  return ... # TODO: complete what the function should return"
      ],
      "metadata": {
        "id": "xxUDdl26WdNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2AFE0zwYcey"
      },
      "source": [
        "### Part (b) -- 8%\n",
        "\n",
        "We want to visualize how good your model is. Therefore, display the initial and boundary conditions data and the predicted data from the model output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAgIS0szWIkj"
      },
      "outputs": [],
      "source": [
        "# TODO: your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discuss your results:**\n"
      ],
      "metadata": {
        "id": "PWJn0l5mRJ2l"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}